{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcb5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import TypedDict , Annotated , Literal\n",
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import InMemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff0e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:3b\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592639ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d137ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(state:JokeState):\n",
    "\n",
    "    prompt = f\"\"\" \n",
    "    Generate a joke about {state['topic']}. \n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    state['joke'] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a42385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_joke(state:JokeState):\n",
    "    prompt = f\"\"\" \n",
    "    Explain the joke {state['joke']}. \n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    state['explanation'] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb10871",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(JokeState)\n",
    "\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"explain_joke\", explain_joke)\n",
    "\n",
    "graph.add_edge(START,\"generate_joke\")\n",
    "graph.add_edge(\"generate_joke\", \"explain_joke\")\n",
    "graph.add_edge(\"explain_joke\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "workflow = graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f721c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'pizza',\n",
       " 'joke': AIMessage(content='Why did the pizza go to therapy?\\n\\nBecause it was feeling crusty!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:48.4853266Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3727034400, 'load_duration': 3434844900, 'prompt_eval_count': 35, 'prompt_eval_duration': 43513000, 'eval_count': 16, 'eval_duration': 224133000, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--11ada263-8eca-4442-91e8-59e238869d83-0', usage_metadata={'input_tokens': 35, 'output_tokens': 16, 'total_tokens': 51}),\n",
       " 'explanation': AIMessage(content='This is a joke response from a language model, specifically a llama (a type of AI model). The joke is:\\n\\n\"Why did the pizza go to therapy?\\nBecause it was feeling crusty!\"\\n\\nThe punchline relies on a play on words. \"Crusty\" has a double meaning here:\\n\\n1. In baking, \"crust\" refers to the outer layer of dough in a pastry or bread.\\n2. Emotionally, \"crusty\" can also mean gruff, bitter, or irritable.\\n\\nSo, the joke is making a pun on these two meanings, suggesting that the pizza went to therapy because it was feeling irritable (or \"crusty\" in an emotional sense), but also referencing the literal crust of the pizza. This type of wordplay is common in jokes and is often used to create humor.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:51.7829998Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3283012800, 'load_duration': 170737900, 'prompt_eval_count': 238, 'prompt_eval_duration': 76201100, 'eval_count': 168, 'eval_duration': 2743807400, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--19020211-992f-489c-a46f-8f73deab4ead-0', usage_metadata={'input_tokens': 238, 'output_tokens': 168, 'total_tokens': 406})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "workflow.invoke ({\n",
    "    \"topic\": \"pizza\"\n",
    "}, config=config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d73887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'topic': 'pizza', 'joke': AIMessage(content='Why did the pizza go to therapy?\\n\\nBecause it was feeling crusty!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:48.4853266Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3727034400, 'load_duration': 3434844900, 'prompt_eval_count': 35, 'prompt_eval_duration': 43513000, 'eval_count': 16, 'eval_duration': 224133000, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--11ada263-8eca-4442-91e8-59e238869d83-0', usage_metadata={'input_tokens': 35, 'output_tokens': 16, 'total_tokens': 51}), 'explanation': AIMessage(content='This is a joke response from a language model, specifically a llama (a type of AI model). The joke is:\\n\\n\"Why did the pizza go to therapy?\\nBecause it was feeling crusty!\"\\n\\nThe punchline relies on a play on words. \"Crusty\" has a double meaning here:\\n\\n1. In baking, \"crust\" refers to the outer layer of dough in a pastry or bread.\\n2. Emotionally, \"crusty\" can also mean gruff, bitter, or irritable.\\n\\nSo, the joke is making a pun on these two meanings, suggesting that the pizza went to therapy because it was feeling irritable (or \"crusty\" in an emotional sense), but also referencing the literal crust of the pizza. This type of wordplay is common in jokes and is often used to create humor.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:51.7829998Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3283012800, 'load_duration': 170737900, 'prompt_eval_count': 238, 'prompt_eval_duration': 76201100, 'eval_count': 168, 'eval_duration': 2743807400, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--19020211-992f-489c-a46f-8f73deab4ead-0', usage_metadata={'input_tokens': 238, 'output_tokens': 168, 'total_tokens': 406})}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-6923-684b-8002-ddd2b542654e'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-30T12:55:51.783636+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-49ce-6861-8001-3aba9923a0c4'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.get_state(config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "921d1826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'topic': 'pizza', 'joke': AIMessage(content='Why did the pizza go to therapy?\\n\\nBecause it was feeling crusty!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:48.4853266Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3727034400, 'load_duration': 3434844900, 'prompt_eval_count': 35, 'prompt_eval_duration': 43513000, 'eval_count': 16, 'eval_duration': 224133000, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--11ada263-8eca-4442-91e8-59e238869d83-0', usage_metadata={'input_tokens': 35, 'output_tokens': 16, 'total_tokens': 51}), 'explanation': AIMessage(content='This is a joke response from a language model, specifically a llama (a type of AI model). The joke is:\\n\\n\"Why did the pizza go to therapy?\\nBecause it was feeling crusty!\"\\n\\nThe punchline relies on a play on words. \"Crusty\" has a double meaning here:\\n\\n1. In baking, \"crust\" refers to the outer layer of dough in a pastry or bread.\\n2. Emotionally, \"crusty\" can also mean gruff, bitter, or irritable.\\n\\nSo, the joke is making a pun on these two meanings, suggesting that the pizza went to therapy because it was feeling irritable (or \"crusty\" in an emotional sense), but also referencing the literal crust of the pizza. This type of wordplay is common in jokes and is often used to create humor.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:51.7829998Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3283012800, 'load_duration': 170737900, 'prompt_eval_count': 238, 'prompt_eval_duration': 76201100, 'eval_count': 168, 'eval_duration': 2743807400, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--19020211-992f-489c-a46f-8f73deab4ead-0', usage_metadata={'input_tokens': 238, 'output_tokens': 168, 'total_tokens': 406})}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-6923-684b-8002-ddd2b542654e'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-30T12:55:51.783636+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-49ce-6861-8001-3aba9923a0c4'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'topic': 'pizza', 'joke': AIMessage(content='Why did the pizza go to therapy?\\n\\nBecause it was feeling crusty!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:48.4853266Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3727034400, 'load_duration': 3434844900, 'prompt_eval_count': 35, 'prompt_eval_duration': 43513000, 'eval_count': 16, 'eval_duration': 224133000, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--11ada263-8eca-4442-91e8-59e238869d83-0', usage_metadata={'input_tokens': 35, 'output_tokens': 16, 'total_tokens': 51})}, next=('explain_joke',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-49ce-6861-8001-3aba9923a0c4'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-11-30T12:55:48.498236+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-2600-6eb6-8000-4036443f2098'}}, tasks=(PregelTask(id='9a957010-07bf-3a8f-021b-154afcd7d7ce', name='explain_joke', path=('__pregel_pull', 'explain_joke'), error=None, interrupts=(), state=None, result={'topic': 'pizza', 'joke': AIMessage(content='Why did the pizza go to therapy?\\n\\nBecause it was feeling crusty!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:48.4853266Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3727034400, 'load_duration': 3434844900, 'prompt_eval_count': 35, 'prompt_eval_duration': 43513000, 'eval_count': 16, 'eval_duration': 224133000, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--11ada263-8eca-4442-91e8-59e238869d83-0', usage_metadata={'input_tokens': 35, 'output_tokens': 16, 'total_tokens': 51}), 'explanation': AIMessage(content='This is a joke response from a language model, specifically a llama (a type of AI model). The joke is:\\n\\n\"Why did the pizza go to therapy?\\nBecause it was feeling crusty!\"\\n\\nThe punchline relies on a play on words. \"Crusty\" has a double meaning here:\\n\\n1. In baking, \"crust\" refers to the outer layer of dough in a pastry or bread.\\n2. Emotionally, \"crusty\" can also mean gruff, bitter, or irritable.\\n\\nSo, the joke is making a pun on these two meanings, suggesting that the pizza went to therapy because it was feeling irritable (or \"crusty\" in an emotional sense), but also referencing the literal crust of the pizza. This type of wordplay is common in jokes and is often used to create humor.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:51.7829998Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3283012800, 'load_duration': 170737900, 'prompt_eval_count': 238, 'prompt_eval_duration': 76201100, 'eval_count': 168, 'eval_duration': 2743807400, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--19020211-992f-489c-a46f-8f73deab4ead-0', usage_metadata={'input_tokens': 238, 'output_tokens': 168, 'total_tokens': 406})}),), interrupts=()),\n",
       " StateSnapshot(values={'topic': 'pizza'}, next=('generate_joke',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-2600-6eb6-8000-4036443f2098'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-11-30T12:55:44.744005+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-25e4-6c44-bfff-dc1a0746a8dd'}}, tasks=(PregelTask(id='7a6ca861-2908-efb4-7331-8f2c46ab71dc', name='generate_joke', path=('__pregel_pull', 'generate_joke'), error=None, interrupts=(), state=None, result={'topic': 'pizza', 'joke': AIMessage(content='Why did the pizza go to therapy?\\n\\nBecause it was feeling crusty!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:55:48.4853266Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3727034400, 'load_duration': 3434844900, 'prompt_eval_count': 35, 'prompt_eval_duration': 43513000, 'eval_count': 16, 'eval_duration': 224133000, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--11ada263-8eca-4442-91e8-59e238869d83-0', usage_metadata={'input_tokens': 35, 'output_tokens': 16, 'total_tokens': 51})}),), interrupts=()),\n",
       " StateSnapshot(values={}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdebe-25e4-6c44-bfff-dc1a0746a8dd'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-11-30T12:55:44.732473+00:00', parent_config=None, tasks=(PregelTask(id='0a13dfd9-c879-4a71-176e-22e283573a88', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'topic': 'pizza'}),), interrupts=())]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(workflow.get_state_history(config1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80918759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'pasta',\n",
       " 'joke': AIMessage(content='Why did the spaghetti refuse to get married?\\n\\nBecause it was afraid of getting tangled up in a relationship!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:59:43.6572163Z', 'done': True, 'done_reason': 'stop', 'total_duration': 791847500, 'load_duration': 224681500, 'prompt_eval_count': 35, 'prompt_eval_duration': 223419200, 'eval_count': 22, 'eval_duration': 299409800, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--1dd49bdf-f1f5-49be-8317-686a07b6c831-0', usage_metadata={'input_tokens': 35, 'output_tokens': 22, 'total_tokens': 57}),\n",
       " 'explanation': AIMessage(content='This is a response from an AI model, likely a large language model such as LLaMA. The response includes metadata about the model\\'s performance and the input and output of the conversation.\\n\\nThe content of the response is a simple joke:\\n\\n\"Why did the spaghetti refuse to get married?\\n\\nBecause it was afraid of getting tangled up in a relationship!\"\\n\\nThis joke relies on wordplay, using the common phrase \"tangled up\" (meaning involved or entwined) and applying it to a physical property of spaghetti (getting tangled). The punchline is a play on words, using the multiple meanings of \"tangled\" to create a humorous effect.\\n\\nThe additional metadata includes:\\n\\n* `additional_kwargs`: This key is not used in this response. It might be intended for future use or for different types of responses.\\n* `response_metadata`: This section provides information about the model\\'s performance during this conversation. The metrics include:\\n + `model`: The version of the LLaMA model used (3.2:3b).\\n + `created_at`: The timestamp when the response was generated (2025-11-30T12:59:43.6572163Z).\\n + `done`: A boolean indicating whether the conversation has ended (True).\\n + `done_reason`: The reason why the conversation ended (stop).\\n + `total_duration`, `load_duration`, `prompt_eval_count`, `prompt_eval_duration`, `eval_count`, and `eval_duration`: Times for various stages of processing the input prompt.\\n + `logprobs`: Not used in this response, but might be used in other contexts.\\n + `model_name` and `model_provider`: The name of the model and its provider (ollama).\\n* `id`: A unique identifier for this conversation run.\\n* `usage_metadata`: Provides information about the input and output of the conversation:\\n + `input_tokens`: The number of tokens in the input prompt.\\n + `output_tokens`: The number of tokens in the response.\\n + `total_tokens`: The total number of tokens processed during this conversation.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:59:51.8080888Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8146145000, 'load_duration': 198923700, 'prompt_eval_count': 245, 'prompt_eval_duration': 63770200, 'eval_count': 417, 'eval_duration': 7149129900, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--899fcb61-5f4e-4ee3-8dde-4cf27afa44bc-0', usage_metadata={'input_tokens': 245, 'output_tokens': 417, 'total_tokens': 662})}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = {\"configurable\":{\"thread_id\":\"2\"}}\n",
    "workflow.invoke({\n",
    "    \"topic\": \"pasta\"\n",
    "},config=config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d84e033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'topic': 'pasta', 'joke': AIMessage(content='Why did the spaghetti refuse to get married?\\n\\nBecause it was afraid of getting tangled up in a relationship!', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:59:43.6572163Z', 'done': True, 'done_reason': 'stop', 'total_duration': 791847500, 'load_duration': 224681500, 'prompt_eval_count': 35, 'prompt_eval_duration': 223419200, 'eval_count': 22, 'eval_duration': 299409800, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--1dd49bdf-f1f5-49be-8317-686a07b6c831-0', usage_metadata={'input_tokens': 35, 'output_tokens': 22, 'total_tokens': 57}), 'explanation': AIMessage(content='This is a response from an AI model, likely a large language model such as LLaMA. The response includes metadata about the model\\'s performance and the input and output of the conversation.\\n\\nThe content of the response is a simple joke:\\n\\n\"Why did the spaghetti refuse to get married?\\n\\nBecause it was afraid of getting tangled up in a relationship!\"\\n\\nThis joke relies on wordplay, using the common phrase \"tangled up\" (meaning involved or entwined) and applying it to a physical property of spaghetti (getting tangled). The punchline is a play on words, using the multiple meanings of \"tangled\" to create a humorous effect.\\n\\nThe additional metadata includes:\\n\\n* `additional_kwargs`: This key is not used in this response. It might be intended for future use or for different types of responses.\\n* `response_metadata`: This section provides information about the model\\'s performance during this conversation. The metrics include:\\n + `model`: The version of the LLaMA model used (3.2:3b).\\n + `created_at`: The timestamp when the response was generated (2025-11-30T12:59:43.6572163Z).\\n + `done`: A boolean indicating whether the conversation has ended (True).\\n + `done_reason`: The reason why the conversation ended (stop).\\n + `total_duration`, `load_duration`, `prompt_eval_count`, `prompt_eval_duration`, `eval_count`, and `eval_duration`: Times for various stages of processing the input prompt.\\n + `logprobs`: Not used in this response, but might be used in other contexts.\\n + `model_name` and `model_provider`: The name of the model and its provider (ollama).\\n* `id`: A unique identifier for this conversation run.\\n* `usage_metadata`: Provides information about the input and output of the conversation:\\n + `input_tokens`: The number of tokens in the input prompt.\\n + `output_tokens`: The number of tokens in the response.\\n + `total_tokens`: The total number of tokens processed during this conversation.', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-11-30T12:59:51.8080888Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8146145000, 'load_duration': 198923700, 'prompt_eval_count': 245, 'prompt_eval_duration': 63770200, 'eval_count': 417, 'eval_duration': 7149129900, 'logprobs': None, 'model_name': 'llama3.2:3b', 'model_provider': 'ollama'}, id='lc_run--899fcb61-5f4e-4ee3-8dde-4cf27afa44bc-0', usage_metadata={'input_tokens': 245, 'output_tokens': 417, 'total_tokens': 662})}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdec7-5a33-67c0-8002-a1c3448cfbb8'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-30T12:59:51.809222+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f0cdec7-0c78-6232-8001-8e5c82ffde87'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.get_state(config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296b798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001C585159E10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46fef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
