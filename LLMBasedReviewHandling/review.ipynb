{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad02849e",
   "metadata": {},
   "source": [
    "libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4ed7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import TypedDict , Annotated , Literal\n",
    "from pydantic import BaseModel,Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d281fe",
   "metadata": {},
   "source": [
    "model local llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651674da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# LLM SETUP\n",
    "# -----------------------\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a66eb",
   "metadata": {},
   "source": [
    "1. sentiment extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d46010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# SENTIMENT SCHEMA\n",
    "# -----------------------\n",
    "class SentimentSchema(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "\n",
    "structured_model = model.with_structured_output(SentimentSchema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c073a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "what is the sentiment of this review - The software is good\n",
    "\"\"\"\n",
    "structured_model.invoke(prompt).sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a8fab",
   "metadata": {},
   "source": [
    "define state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9363a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# STATE\n",
    "# -----------------------\n",
    "class ReviewState(TypedDict):\n",
    "    review: str\n",
    "    sentiment: Literal[\"positive\", \"negative\"]\n",
    "    diagnosis: dict\n",
    "    response: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c9afe",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0979772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(state: ReviewState) -> ReviewState:\n",
    "    prompt = f\"\"\"\n",
    "    For the following review, find the sentiment:\n",
    "\n",
    "    \"{state['review']}\"\n",
    "    \"\"\"\n",
    "\n",
    "    result = structured_model.invoke(prompt).sentiment\n",
    "\n",
    "    return {**state, \"sentiment\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55afed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sentiment(state: ReviewState) -> Literal[\"positive_response\", \"run_diagnosis\"]:\n",
    "    if state[\"sentiment\"] == \"positive\":\n",
    "        return \"positive_response\"\n",
    "    else:\n",
    "        return \"run_diagnosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdde1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_response(state: ReviewState) -> ReviewState:\n",
    "    prompt = f\"\"\"\n",
    "    Write a warm, friendly thank-you message in response to this review:\n",
    "\n",
    "    \"{state['review']}\"\n",
    "\n",
    "    Also ask the user to leave feedback on our website.\n",
    "    \"\"\"\n",
    "\n",
    "    reply = model.invoke(prompt).content\n",
    "\n",
    "    return {**state, \"response\": reply}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c61edd",
   "metadata": {},
   "source": [
    "schema for structured o/p for negative response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a4ff24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisSchema(BaseModel):\n",
    "    issue_type: Literal[\"product\", \"service\", \"pricing\", \"delivery\"] = Field(description=\"The type of issue\")\n",
    "    tone: Literal[\"positive\", \"negative\", \"neutral\",\"calm\", \"angry\"] = Field(description=\"The tone expressed by user for the review\")\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"The urgency of the issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867d6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model2 = model.with_structured_output(DiagnosisSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8790614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diagnosis(state: ReviewState) -> ReviewState:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Diagnose this negative review:\n",
    "\n",
    "    \"{state[\"review\"]}\"\n",
    "\n",
    "    Return issue_type, tone, and urgency.\n",
    "    \"\"\"\n",
    "\n",
    "    response = structured_model2.invoke(prompt)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"diagnosis\": response.model_dump()   \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cb2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_response(state: ReviewState) -> ReviewState:\n",
    "    diagnosis = state[\"diagnosis\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a support assistant.\n",
    "\n",
    "    The user had a \"{diagnosis[\"issue_type\"]}\" issue\n",
    "    with tone \"{diagnosis[\"tone\"]}\" \n",
    "    and urgency \"{diagnosis[\"urgency\"]}\".\n",
    "\n",
    "    Write an empathetic response and provide a helpful resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    output = model.invoke(prompt).content\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"response\": output\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde590d",
   "metadata": {},
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45c1fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(ReviewState)\n",
    "\n",
    "graph.add_node(\"find_sentiment\", find_sentiment)\n",
    "graph.add_node(\"positive_response\", positive_response)\n",
    "graph.add_node(\"run_diagnosis\", run_diagnosis)\n",
    "graph.add_node(\"negative_response\", negative_response)\n",
    "\n",
    "graph.add_edge(START, \"find_sentiment\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"find_sentiment\",\n",
    "    check_sentiment,  # returns \"positive_response\" or \"run_diagnosis\"\n",
    ")\n",
    "\n",
    "graph.add_edge(\"positive_response\", END)\n",
    "graph.add_edge(\"run_diagnosis\", \"negative_response\")\n",
    "graph.add_edge(\"negative_response\", END)\n",
    "\n",
    "workflow = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa71b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"review\": \"The product was delayed and support did not respond.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ee22e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"review\": \"The product is good.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90ffd4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55eae79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': 'The product was delayed and support did not respond.', 'sentiment': 'negative', 'diagnosis': {'issue_type': 'product', 'tone': 'negative', 'urgency': 'low'}, 'response': \"I'm so sorry to hear that you're experiencing issues with your product. I can see that you're frustrated, and I want to assure you that I'm here to help.\\n\\nCan you please tell me more about the issue you're facing? What's not working as expected, and how is it affecting you? I'll do my best to understand the problem and provide a solution.\\n\\nIn the meantime, I'd like to offer some temporary assistance. Would you be open to receiving a replacement part or a refund, depending on your preference? We want to get this resolved for you as soon as possible.\\n\\nPlease know that we value your business and appreciate your feedback. Your concerns are important to us, and we'll do our best to make things right.\\n\\nIf there's anything else I can do to help, please let me know. I'm here to support you, and I want to ensure that you're satisfied with the outcome.\\n\\nYour case number is [insert case number]. If you have any further questions or concerns, feel free to reach out to me directly.\\n\\nThank you for your patience, and I look forward to resolving this issue for you soon.\"}\n"
     ]
    }
   ],
   "source": [
    "print(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc473b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
